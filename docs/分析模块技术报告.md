# 分析模块技术报告

## 1. 概述

分析模块（`src/analyzer`）负责将小说原文转化为结构化知识：在「元协议」指导下逐章/按窗提取知识卡片与剧情节点，经高质量模型整合与冲突检测，产出**小说数据库**（实体、情节树、冲突标记）及**分析状态**，供写作、改写、回溯等下游使用。

**设计原则**：双环分工——长上下文/高质量模型负责「大脑型」任务（协议设计、抽样、冲突检测、整合）；低成本模型负责「劳工型」任务（批量逐章提取），在保证质量的前提下控制 API 成本与延迟。

---

## 2. 架构总览

```
                    ┌─────────────────────────────────────────────────────────┐
                    │                    run_full_pipeline                      │
                    └─────────────────────────────────────────────────────────┘
                                              │
         ┌───────────────────────────────────┼───────────────────────────────────┐
         ▼                                   ▼                                   ▼
┌─────────────────────┐           ┌─────────────────────┐           ┌─────────────────────┐
│ Phase1 采样与元协议   │           │ Phase2 逐章/窗口提取  │           │ 输出与持久化         │
│ run_phase1_*         │           │ run_phase2_*         │           │ save_state          │
│ · smart_sample       │  ──────►  │ · extract_cards_*    │  ──────►  │ build_novel_database│
│ · generate_meta_     │   state   │ · detect_conflicts_  │   state   │ novel_database.json  │
│   protocol           │           │   and_merge           │           │ style_fingerprint   │
└─────────────────────┘           └─────────────────────┘           └─────────────────────┘
         │                                   │
         │ 长上下文/高质量                     │ 低成本(逐章) + 高质量(整合)
         ▼                                   ▼
   ANALYZER_LONG_CONTEXT_MODEL          ANALYZER_LOW_MODEL (默认 glm-4.5-air)
   ANALYZER_HIGH_MODEL                  ANALYZER_HIGH_MODEL (冲突检测/合并)
```

**入口**：`main.py analyze`（完整流程）或 `analyze --re-extract`（仅重跑 Phase2，沿用已有元协议）。

---

## 3. 核心子模块

### 3.1 状态与数据模型（state_schema.py / models.py）

| 类型 | 说明 |
|------|------|
| **AnalysisState** | 全局分析状态：book_id、title、meta_protocol、sampled_*、cards、plot_tree、conflict_marks、pending_patches、template_version、last_processed_chapter_index 等。 |
| **MetaProtocol** | 元协议：logic_red_lines（逻辑红线）、element_template（知识卡片字段定义）、term_mapping（术语规范）、note。 |
| **KnowledgeCard** | 知识卡片：type、name、description、first_chapter_id、attributes（按 element_template 的扩展字段）、plot_node_ids。 |
| **PlotNode** | 剧情节点：id、type、summary、chapter_id、chapter_index、parent_id、cause_effect_notes，用于构建因果树。 |
| **ConflictMark** | 冲突标记：conflict_type、description、card_or_node_ids、chapter_ids、suggestion。 |
| **PendingPatch** | 补丁：chapter_id、chapter_index、issue、suggestion，供战略层重构元协议。 |
| **NovelDatabase** | 小说数据库：entities_by_type（设定/道具/场景/角色/事件）、plot_tree、cards、conflict_marks、meta_protocol。 |

**未分类设定**：`UNCLASSIFIED_FIELD_NAME = "未分类设定"`，element_template 默认包含该字段，用于收纳低质量模型无法归入已有字段的新设定，减少补丁爆炸。

---

### 3.2 智能抽样（sampler.py）

- **输入**：全书 chapters、可选 total_chapter_count。
- **逻辑**：根据总章数计算抽样比例（约 10%–20%，章少多抽、章多少抽），构建「全书目录 + 前 50 章摘要」文本，调用**高质量模型**（ANALYZER_HIGH_MODEL）选出关键节点章节序号（1-based），去重排序后转为 0-based 索引及 chapter_id 列表。
- **输出**：`(sampled_indices_0based, sampled_chapter_ids)`，供元协议生成使用。

---

### 3.3 元协议生成（protocol_generator.py）

- **输入**：book_id、book_title、chapters、sampled_indices_0based、use_long_context。
- **流程**：
  1. **首轮**：用 `_build_sampled_content` 将采样章节拼成文本（单章/总长上限控制），构造 prompt，调用 **长上下文模型**（ANALYZER_LONG_CONTEXT_MODEL，默认 Kimi K2 Turbo 预览）或高质量模型生成元协议 JSON；解析失败时带错误原因重试（最多 META_PROTOCOL_MAX_RETRIES 次）；长上下文返回空时降级为高质量模型 + 缩短采样内容重试。
  2. **多轮优化**：首轮解析成功后，按「每轮 3 章」遍历采样章节，每轮仅发送「当前元协议 JSON + 本轮 3 章」到**高质量模型**（ANALYZER_HIGH_MODEL，如 DeepSeek），避免历史累积超过 128k token；轮数 = min(ceil(采样章数/3), MAX_REFINEMENT_ROUNDS)。
- **输出**：MetaProtocol（logic_red_lines、element_template、term_mapping、note）。
- **鲁棒性**：兼容中文键（逻辑红线、要素模版等）、从回复中提取 JSON（去 markdown 包裹）、总长与单轮上下文上限（MAX_SAMPLED_CONTENT_CHARS、MAX_REFINEMENT_USER_CHARS）控制。

---

### 3.4 逐章/窗口提取（extractor.py）

- **逐章**：`extract_cards_from_chapter(chapter_title, content, chapter_id, chapter_index, protocol, existing_node_ids)`  
  在元协议指导下，将单章正文转为「知识卡片 + 剧情节点」JSON，使用**低成本模型**（ANALYZER_LOW_MODEL，默认智谱 glm-4.5-air）；返回空时可选降级为高质量模型。
- **窗口**：`extract_cards_from_window(window_chapters, protocol, existing_node_ids, window_start_index)`  
  对多章窗口一次性提取，同样使用低成本模型。
- **输出**：`(List[KnowledgeCard], List[PlotNode])`；解析时兼容中英文键（知识卡片/剧情节点），未覆盖内容写入 attributes["未分类设定"]。

---

### 3.5 冲突检测与合并（refiner.py）

- **接口**：`detect_conflicts_and_merge(state, new_cards, new_nodes)`。
- **逻辑**：用**高质量模型**对比「已有状态摘要」与「本轮新卡片/节点」，判断是否存在逻辑矛盾（如角色复活、时间线冲突、设定矛盾）；若存在则输出 conflicts 数组并转为 ConflictMark；无论是否冲突，将 new_cards/new_nodes 合并进 state（cards 追加、plot_tree 按 id 写入、conflict_marks 追加）。
- **用途**：逐章或窗口模式下，每批新提取结果在合并前经此步骤，保证因果一致性与冲突可追溯。

---

### 3.6 补丁与战略层重构（patch_refactor.py）

- **Patch-Buffer**：低质量模型在提取时若发现「无法归入现有字段」的设定，通过 `submit_patch(state, chapter_id, chapter_index, issue, suggestion)` 写入 `state.pending_patches`，不直接改主模版。
- **模版健康度**：`template_health_check(protocol, pending_count, extraction_stats)` 检查 token 估算、补丁数量阈值、字段空置率等，返回是否建议重构（should_refactor）。
- **重构**：`refactor_meta_protocol_with_patches(protocol, patches, sample_failures, ...)` 由高质量模型根据当前模版 + 补丁 + 典型失败 case 输出新模版 V2；`try_refactor_if_needed(state, chapter_index)` 在 pipeline 中按 REFACTOR_CHAPTER_INTERVAL / PATCH_THRESHOLD 等条件触发，更新 state.meta_protocol 并清空/裁剪 pending_patches。
- **流水线集成**：逐章整合后对每章调用 `collect_patches_from_cards(cards, chapter_id, chapter_index)` 收集补丁并 `submit_patch`，再 `try_refactor_if_needed`。

---

### 3.7 流水线编排（pipeline.py）

| 函数 | 说明 |
|------|------|
| **run_phase1_sampling_and_protocol** | 创建初始 state；smart_sample → 写 state.sampled_*；generate_meta_protocol → 写 state.meta_protocol。 |
| **run_phase2_concurrent_extract_then_consolidate** | 并发调用 extract_cards_from_chapter（线程池），再按章序依次 detect_conflicts_and_merge、collect_patches、submit_patch、try_refactor_if_needed；带「提取章节」「整合章节」进度条。 |
| **run_phase2_per_chapter_then_consolidate** | 逐章串行：每章 extract → merge → patch 收集与 try_refactor；带「分析章节」进度条。 |
| **run_phase2_incremental_extract** | 窗口模式：按 WINDOW_SIZE=3 滑动，每窗 extract_cards_from_window → refine_after_window；带「窗口提取」进度条。 |
| **run_full_pipeline** | Phase1 + Phase2（按参数选择逐章并发/逐章串行/窗口）；顶层「分析流程」两阶段进度条。 |
| **run_phase2_only** | 加载已有 state，清空 cards/plot_tree/conflict_marks/pending_patches 等，仅执行 Phase2；用于「只重跑章节提取、元协议沿用」。 |

---

## 4. 模型与配置

| 环境变量 | 用途 | 默认值 |
|----------|------|--------|
| **ANALYZER_LONG_CONTEXT_MODEL** | 元协议首轮生成（长文本） | kimi-k2-turbo-preview（Kimi K2 Turbo 预览） |
| **ANALYZER_HIGH_MODEL** | 元协议优化轮、智能抽样、冲突检测与合并、补丁重构、逻辑回溯等 | deepseek-reasoner |
| **ANALYZER_LOW_MODEL** | 逐章/窗口提取 | glm-4.5-air（智谱 GLM-4.5-Air） |
| **ANALYZER_FAST_MODEL** | 可选；提取时若配置则优先用于加速 JSON 抽取 | 未配置则用 ANALYZER_LOW_MODEL |

统一通过 `src/utils/llm_client.py` 的 `chat_long_context`、`chat_high_quality`、`chat_low_cost` 调用；支持 DeepSeek、Moonshot（Kimi）、智谱、OpenAI，模型名与 API 映射及别名见 llm_client。

---

## 5. 进度与可观测性

- **分析流程**：两阶段进度条（Phase1 采样与元协议 → Phase2 逐章/窗口提取与整合）。
- **元协议优化**：多轮优化时每轮进度（元协议优化 1/N 轮）。
- **章节/窗口**：逐章模式为「分析章节」或「提取章节」+「整合章节」；窗口模式为「窗口提取」。
- 依赖可选库 `tqdm`；未安装时跳过进度条，逻辑不变。

---

## 6. 输出产物

- **analysis_state.json**：完整 AnalysisState（含 meta_protocol、cards、plot_tree、conflict_marks、pending_patches 等），供改写、回溯、re-extract 使用。
- **novel_database.json**：NovelDatabase（entities_by_type、plot_tree、cards、conflict_marks、meta_protocol），面向应用的结构化知识库。
- **style_fingerprint.json**：由 librarian 模块基于原文构建的风格指纹库（与分析状态同目录）。

---

## 7. 扩展与依赖

- **改写链路**：rewrite_pipeline 使用 load_state_for_rewrite、step1_impact_assessment（causal_tracker）、step2_dynamic_context、step3_double_check、step4_incremental_sync。
- **回溯**：backtrack.check_consistency、query_causal_parents 基于 state.cards 与 state.plot_tree。
- **演化/写作**：evolution、writer 等模块依赖 AnalysisState、KnowledgeCard、PlotNode、MetaProtocol 等类型及 build_novel_database。
- **仿写/逆向重构流水线**：`main.py full-pipeline --reconstruction` 的 Phase 0 使用本模块产出的 `novel_database.json`、`style_fingerprint.json`；Phase 1 从情节树构建 `reconstructed_outline.json`，Phase 2 按大纲多打字机并行渲染并后台入库。若已有大纲，可用 `--skip-framework` 跳过 Phase 1，直接跑 Phase 2（见 `docs/使用报告.md`）。

---

## 8. 小结

分析模块采用「智能抽样 → 元协议（长上下文/高质量）→ 低成本逐章/窗口提取 → 高质量整合与冲突检测」的双环流水线，通过元协议、补丁缓冲与战略层重构控制模版演化，通过未分类设定与冲突标记保证可扩展性与可追溯性。章节提取默认使用智谱 GLM-4.5-Air，长上下文默认使用 Kimi K2 Turbo 预览，高质量任务使用 ANALYZER_HIGH_MODEL（如 DeepSeek）；支持仅重跑 Phase2（--re-extract）以在保持元协议不变的前提下重新提取整书。
